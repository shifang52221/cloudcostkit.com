---
import GuideLayout from "../../layouts/GuideLayout.astro";

const title = "DynamoDB pricing: what to model (reads, writes, storage, extras)";
const description =
  "A practical DynamoDB pricing checklist: model reads and writes (RCU/WCU), storage (GB-month), and the common add-ons (backups, streams, global tables). Includes pitfalls and validation steps.";
---
<GuideLayout
  title={title}
  description={description}
  canonicalPath="/guides/aws-dynamodb-pricing"
  lastUpdated="2026-01-27"
  faqs={[
    {
      q: "What are the main DynamoDB cost drivers?",
      a: "Read/write capacity (or on-demand request volume) plus storage. Item size and access patterns can change the effective read/write units materially.",
    },
    {
      q: "What add-ons commonly surprise teams?",
      a: "Backups/PITR, streams, and global tables. They can become meaningful at scale and should be modeled explicitly.",
    },
  ]}
>
  <p>
    DynamoDB pricing is easiest when you model it as a few buckets: <strong>reads</strong>, <strong>writes</strong>,
    <strong>storage</strong>, and <strong>extras</strong>. Most estimation errors come from ignoring item size effects or
    forgetting secondary indexes and replication features.
  </p>

  <h2>1) Reads and writes: count the billable units</h2>
  <ul>
    <li>
      <strong>Request volume</strong>: reads/day and writes/day (or requests/second by workload type).
    </li>
    <li>
      <strong>Item size</strong>: larger items consume more units per operation.
    </li>
    <li>
      <strong>Access pattern</strong>: strongly consistent reads, scans, and transactional ops can cost more.
    </li>
  </ul>
  <p class="muted">
    Related: <a href="/guides/aws-dynamodb-rcu-wcu-explained/">RCU/WCU explained</a>.
  </p>
  <div class="btn-row">
    <a class="btn" href="/calculators/aws-dynamodb-cost-calculator/">DynamoDB cost calculator</a>
    <a class="btn" href="/calculators/rps-to-monthly-requests-calculator/">RPS to requests</a>
  </div>

  <h2>2) Storage: GB-month and data shape</h2>
  <ul>
    <li>
      <strong>Table storage</strong>: GB-month based on item size and item count.
    </li>
    <li>
      <strong>Indexes</strong>: global secondary indexes (GSIs) can multiply stored bytes.
    </li>
    <li>
      <strong>Retention policies</strong>: TTL and archival strategies reduce long-term storage growth.
    </li>
  </ul>

  <h2>How to estimate item size (so the model matches reality)</h2>
  <ul>
    <li>Measure average and p95 item size from real items (not just schema guesses).</li>
    <li>Separate "hot path" items from "cold path" items (their sizes can differ significantly).</li>
    <li>Estimate index storage separately: projected attributes often create an extra copy of data.</li>
  </ul>
  <p class="muted">
    If you cannot measure yet, build a conservative range and validate as soon as you have real table data.
  </p>

  <h2>3) Common extras (do not forget these)</h2>
  <ul>
    <li>
      <strong>Backups / PITR</strong>: continuous backup and restore requirements can add a meaningful baseline.
    </li>
    <li>
      <strong>Streams</strong>: if enabled, include downstream processing and storage.
    </li>
    <li>
      <strong>Global tables</strong>: replication can add write/transfer/storage effects across regions.
    </li>
  </ul>

  <h2>Provisioned vs on-demand (what changes in the estimate)</h2>
  <ul>
    <li>
      <strong>Provisioned</strong>: model sustained capacity (and autoscaling behavior) and include headroom for busy months.
    </li>
    <li>
      <strong>On-demand</strong>: model request volume and include incident multipliers (retries increase cost).
    </li>
    <li>
      For either mode, item size and index amplification still matter.
    </li>
  </ul>

  <h2>A fast estimation workflow</h2>
  <ol>
    <li>Measure read and write request volume (baseline + busy month).</li>
    <li>Measure typical item sizes (average and p95).</li>
    <li>Model indexes (how many and what attributes they project).</li>
    <li>Add extras: backups/PITR, streams, replication.</li>
  </ol>

  <h2>Common pitfalls</h2>
  <ul>
    <li>Using “requests” without converting to RCU/WCU based on item size and consistency.</li>
    <li>Ignoring scans and large responses (they can dominate read units).</li>
    <li>Forgetting that GSIs add storage and write amplification.</li>
    <li>Modeling only steady state and ignoring incident retry storms.</li>
  </ul>

  <h2>Validation checklist</h2>
  <ul>
    <li>Validate request counts from a real week of usage (not only peak RPS).</li>
    <li>Validate item sizes from real items (average and p95).</li>
    <li>Validate index count and projections; treat them as separate storage/write multipliers.</li>
    <li>Validate whether you require PITR, streams, or multi-region replication.</li>
  </ul>

  <h2>Sources</h2>
  <ul>
    <li>
      DynamoDB pricing:{" "}
      <a href="https://aws.amazon.com/dynamodb/pricing/" rel="nofollow noopener" target="_blank">
        aws.amazon.com/dynamodb/pricing
      </a>
    </li>
    <li>
      DynamoDB developer guide:{" "}
      <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html" rel="nofollow noopener" target="_blank">
        docs.aws.amazon.com
      </a>
    </li>
  </ul>
</GuideLayout>
