---
import GuideLayout from "../../layouts/GuideLayout.astro";

const title = "AWS Lambda pricing (what to include)";
const description =
  "A practical checklist for estimating AWS Lambda-style costs: requests, duration × memory (GB-seconds), provisioned concurrency when used, logs, and common hidden line items.";
const faqs = [
  {
    q: "What are the two core Lambda pricing inputs?",
    a: "Invocations (request count) and compute usage in GB-seconds (duration × memory).",
  },
  {
    q: "Do logs and metrics come with Lambda?",
    a: "Typically no. Logging and metrics are separate services (for example, CloudWatch) and should be modeled as separate line items.",
  },
  {
    q: "How do I compare Lambda to containers/VMs?",
    a: "Use unit economics: cost per 1M requests (or cost per request) for your expected duration and memory. Then compare to a steady-state monthly compute budget for containers/VMs.",
  },
];
---
<GuideLayout title={title} description={description} canonicalPath="/guides/aws-lambda-pricing" lastUpdated="2026-01-27" faqs={faqs}>
  <p>
    Lambda pricing looks simple on paper, but budgets get surprised by the surrounding line items: logs, data transfer,
    retries, and downstream services. Use this checklist to build a realistic estimate and validate it after you deploy.
  </p>

  <h2>1) Start with the two core line items</h2>
  <ul>
    <li><strong>Requests</strong>: billed per invocation (often priced per 1M requests).</li>
    <li><strong>Compute</strong>: billed as <strong>GB-seconds</strong> (duration × configured memory).</li>
  </ul>
  <p class="muted">
    Tool: <a href="/calculators/aws-lambda-cost-calculator/">AWS Lambda cost calculator</a>
  </p>

  <h2>2) Model duration as a range (not a single number)</h2>
  <p>
    Duration usually has a long tail. For planning, keep at least two scenarios:
  </p>
  <ul>
    <li><strong>Typical</strong>: a representative median or steady period.</li>
    <li><strong>Busy/incident</strong>: p95-ish or “known heavy window” where retries, cold starts, or downstream latency increases duration.</li>
  </ul>

  <h2>3) Memory settings directly change compute cost</h2>
  <p>
    Lambda compute cost scales with configured memory, even if your function rarely uses it. Over-allocating memory
    increases GB-seconds linearly. Under-allocating can increase duration and still raise cost.
  </p>
  <ul>
    <li>Right-size memory using measured duration vs memory experiments.</li>
    <li>Watch for “downstream bound” functions: adding memory might not reduce duration if the bottleneck is a database/API.</li>
  </ul>

  <h2>4) Add provisioned concurrency if you use it</h2>
  <p>
    Provisioned concurrency can improve cold-start behavior, but it adds a baseline cost. Treat it like “always-on
    capacity” and model it separately from on-demand invocations.
  </p>
  <p class="muted">
    Guide: <a href="/guides/aws-lambda-concurrency-and-cold-starts/">concurrency and cold starts</a>
  </p>

  <h2>5) Common Lambda-adjacent costs</h2>
  <ul>
    <li><strong>Logging</strong>: ingestion + retention; large JSON logs add up quickly.</li>
    <li><strong>Networking</strong>: NAT/egress and cross-AZ transfer patterns depending on architecture.</li>
    <li><strong>Downstream services</strong>: databases, queues, storage; they often dominate total spend in real systems.</li>
  </ul>
  <div class="btn-row">
    <a class="btn" href="/calculators/log-cost-calculator/">Log cost</a>
    <a class="btn" href="/calculators/vpc-data-transfer-cost-calculator/">Transfer cost</a>
    <a class="btn" href="/calculators/aws-dynamodb-cost-calculator/">DynamoDB cost</a>
  </div>

  <h2>6) Sanity-check with unit economics</h2>
  <p>
    Convert your estimate into “cost per 1M requests” for a typical duration/memory combination. It makes regressions
    obvious and helps compare to always-on compute.
  </p>
  <p class="muted">
    Related: <a href="/guides/aws-lambda-vs-fargate-cost/">Lambda vs Fargate cost</a>
  </p>

  <h2>Common pitfalls</h2>
  <ul>
    <li>Budgeting from a single duration number and ignoring long-tail behavior.</li>
    <li>Over-allocating memory “just in case” without measuring the duration trade-off.</li>
    <li>Retry storms inflating invocation count and total duration during incidents.</li>
    <li>Ignoring log volume and retention until it becomes a top driver.</li>
    <li>Forgetting data transfer and NAT patterns when functions run in a VPC.</li>
  </ul>

  <h2>How to validate after you deploy</h2>
  <ul>
    <li>Confirm billed request count and billed duration match your model for a representative week.</li>
    <li>Check p50/p95 duration and error/retry rates; duration inflation often explains spend spikes.</li>
    <li>Measure log ingestion GB/day and confirm retention settings match your policy.</li>
  </ul>

  <h2>Sources</h2>
  <ul>
    <li>
      <a href="https://aws.amazon.com/lambda/pricing/" target="_blank" rel="nofollow noopener">
        AWS Lambda pricing
      </a>
    </li>
    <li>
      <a href="https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html" target="_blank" rel="nofollow noopener">
        Provisioned concurrency documentation
      </a>
    </li>
  </ul>
</GuideLayout>
